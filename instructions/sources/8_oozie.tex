\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage[polish]{babel}

\usepackage{listings}
\usepackage[T1]{fontenc}

\pagenumbering{gobble}
\lstset{
	language=sql,
	frame=single,
	basicstyle=\tiny,
	literate=*{-}{-}1,
	columns=fullflexible
	}

\begin{document}
\section*{Oozie}

\subsection*{Hadoop Streaming}
\begin{enumerate}
\item wrzuć mapper.py i reducer.py na hdfsa
\item rozwiń na hue dropdown menu przy przycisku 'Query', wejdź w menu 'Scheduler' i kliknij  'Workflow'
\item kliknij w dropdown 'DOCUMENTS' i wybierz 'Actions'
\item Przeciągnij akcję streaming do workflowu
\item W pole Mapper wpisz 'mapper.py', a w pole Reducer wpisz 'reducer.py'
\item Kliknij 'Add'
\item Kliknij w przycisk 'FILES+' i znajdź plik 'mapper.py', następnie kliknij jeszcze raz i znajdź plik 'reducer.py'
\item Kliknij w prawym górnym rogu akcji w przycisk ustawień
\item Kliknij w przycisk 'PROPERTIES+' i wpisz w pierwsze pole 'mapred.input.dir', a w drugim podaj ścieżkę do katalogu / pliku wejściowego
\item Kliknij jeszcze raz i dodaj property 'mapred.output.dir' z namiarami na katalog wyjściowy
\item Zapisz workflow klikając save w prawym górnym rogu
\end{enumerate}

\subsection*{Mapreduce}
\begin{enumerate}
\item wrzuć jara z jobem na hdfsa
\item dodaj akcję 'Java program' do workflowu
\item w polu 'Jar name' znajdź jara z jobem na hdfsie
\item w polu 'Main class' wpisz nazwę klasy razem z pakietem
\item Kliknij 2 razy w 'ARGUMENTS+'. W pierwszym polu wpisz plik/katalog wejściowy, a w drugim wyjściowy
\item Zapisz workflow klikając save w prawym górnym rogu
\end{enumerate}

\subsection*{Sqoop}
\begin{enumerate}
\item wrzuć 'mysql-connector-java-5.1.46-bin.jar' na hdfsa
\item dodaj akcję 'sqoop1' do workflowu
\item w polu "Sqoop command" wpisz komendę do importu sqoop (bez polecenia sqoop) i kliknij add
\item Kliknij w prawym górnym rogu akcji w przycisk ustawień
\item kliknij na 'ARCHIVES+'
\item znajdź na 'hdfsie mysql-connector-java-5.1.46-bin.jar'
\item Zapisz workflow klikając save w prawym górnym rogu
\end{enumerate}

\subsection*{Hive}
\begin{enumerate}
\item stwórz skrypt 'query.sql' zawierający komenty sqlowe do wykonania
\item umieść skrypt 'query.sql' na hdfsie
\item dodaj akcję 'HiveServer2' do workflowu
\item podaj namiary na skrypt 'query.sql' i kliknij 'add'
\item Zapisz workflow klikając save w prawym górnym rogu
\end{enumerate}

\subsection*{Koordynator}
\begin{enumerate}
\item rozwiń dropdown menu przy przycisku 'Query', wejdź w menu 'Scheduler' i kliknij  'Schedule'
\item Kliknij na dropdown 'Choose a workflow...' i wybierz workflow, który chciałbyś zaschedulować.
\item Wybierz jak często job ma się wykonywać. Możesz kliknąć w Options i zaznaczyć "Advanced syntax", żeby wpisać wyrażenie crona.
\item Wybierz zakres dat w jakim scheduler ma się wykonywać
\item kliknij save, żeby zapisać koordynatora
\end{enumerate}

\subsection*{Dodanie parametru do joba}
\begin{lstlisting}
${nazwa_parametru}
\end{lstlisting}

\pagebreak
\subsection*{Zadania}
\begin{enumerate}
\item Utwórz workflow z joba streaming sliczającego słowa i joba mapreduce sortującego po liczbie wystąpień
\item Utwórz workflow ściągający dane ze sqoopa, ładujący je do tabeli 'owners' na hivie i wyliczający tabelę wynikową będącą połączeniem tabeli 'tranfers' z tabelą 'owners'
\end{enumerate}

\end{document}
